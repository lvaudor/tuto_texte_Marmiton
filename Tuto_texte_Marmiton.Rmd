---
title: "Mitonner des beaux petits graphiques à partir des données textuelles du site Marmiton"
output: html_document
---

Dans ce projet je vais

- sélectionner au hasard 100 recettes parmi les environ 9000 recettes de dessert que comptent le site Marmiton
- scraper certaines informations issues de ces fiche-recettes (notamment, les informations concernant les ingrédients de la recette et les commentaires)

# Packages

Voici les packages dont j'aurai besoin pour ce projet

```{r setup, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, cache=TRUE)
library(tidyverse)
library(purrr)
library(rvest)
library(tidytext)
library(proustr)
library(wordcloud)
library(widyr)
library(igraph)
library(ggraph)
```

# Web scraping et mise en forme des données

## Récupérer les urls des listes de recettes de desserts

Si je fais une recherche sur Marmiton pour trouver les fiches-recettes des dessert, il me renvoie une liste de plusieurs pages dont les urls sont de la forme:


- "http://www.marmiton.org/recettes/recherche.aspx?aqt=dessert&dt=dessert&pht=1"
- "http://www.marmiton.org/recettes/recherche.aspx?aqt=dessert&dt=dessert&pht=1&start=12"
- "http://www.marmiton.org/recettes/recherche.aspx?aqt=dessert&dt=dessert&pht=1&start=24"
- "http://www.marmiton.org/recettes/recherche.aspx?aqt=dessert&dt=dessert&pht=1&start=36"
etc.

Je vais donc commencer par créer un vecteur `pages` qui liste l'ensemble des pages, qui elles-mêmes listent les recettes de dessert:

```{r pages}
pages <- str_c("http://www.marmiton.org/recettes/recherche.aspx?aqt=dessert&dt=dessert&pht=1",
               c("",str_c("&start=",seq(12,9000,by=12))))
pages[1:4]
```

## Récupérer les urls des recettes de desserts

Je définis une fonction (`get_recettes()`) qui pour chaque page listée ci-dessus, me récupère l'ensemble des urls pointant vers les recettes: 

```{r recup_recettes, echo=FALSE}
recup_recettes <- function(page){
  html <- read_html(page)
  urls <- html  %>% 
    html_nodes("body") %>% 
    html_nodes(".recipe-card") %>% 
    html_attrs() %>%
    map(.f=function(x){x[[which(names(x)=="href")]]}) %>% 
    unlist()
  tib=tibble(urls=urls)
  return(tib)
}
```

J'applique ensuite itérativement la fonction `get_recettes()` à l'ensemble des éléments du vecteur `pages` défini ci-dessus.

Pour ce tuto, je vais me contenter de 1000 recettes (parmi environ 9000) tirées au hasard...

Le résultat est enregistré dans le fichier "data/recettes_sample.csv".

```{r applique_recup_recettes}
if(!file.exists("data/recettes_sample.csv")){
  recettes <- pages%>%
    map(.f=recup_recettes) %>%
    bind_rows()
  set.seed(33)
  recettes_sample <- sample_n(recettes, 1000)
  write_csv(recettes_sample,"data/recettes_sample.csv")
}
recettes_sample <- read_csv("data/recettes_sample.csv")
head(recettes_sample)
```

## Récupérer la liste des ingrédients

On définit maintenant la fonction `recup_ingredients()` qui permet, à partir de l'url d'une recette, de récupérer un tableau qui liste les ingrédients.

```{r recup_ingredients}
recup_ingredients <- function(url){
    html<-read_html(url)
    # Recupere titre
    recette <- html %>%
      html_nodes(".main-title") %>% 
      html_text()
    # Recupere quantites
    quantites <- html %>%
      html_nodes(".recipe-ingredient-qt")  %>%
      html_text()
    # Recupere ingredients
    ingredients <- html %>%
      html_nodes(".ingredient") %>% 
      html_text()
    # Rassemble le tout dans une tibble 
    tib <- bind_cols(url=rep(url,length(ingredients)),
                   recette=rep(recette, length(ingredients)),
                   quantites=quantites,
                   ingredients=ingredients)
    return(tib)
}

recup_ingredients(recettes_sample$urls[24]) %>%
  select(-url)
```

On applique alors itérativement la fonction `recup_ingredients()` à l'ensemble des recettes listées dans `recettes_sample`.

Le résultat est enregistré dans le fichier "data/tib_ingredients.csv".

```{r applique_recup_ingredients}
if(!file.exists("data/tib_ingredients.csv")){
  tib_ingredients <- map(recettes_sample$urls,
                      .f=recup_ingredients) %>% 
    bind_rows()
  write_csv(tib_ingredients,"data/tib_ingredients.csv")
}
tib_ingredients <- read_csv("data/tib_ingredients.csv")

select(tib_ingredients,-url) %>%
  head(n=15)
```

## Nettoyer les ingrédients

Je nettoie la table `tib_ingredients` de manière à isoler les unités de mesure de l'ingrédient lui-même...

```{r clean_tib_ingredients}
tib_ingredients=tib_ingredients %>% 
  mutate(ingredients=str_replace(ingredients,"cuillère à soupe","CàS")) %>% 
  mutate(ingredients=str_replace(ingredients,"cuillère à café","CàC"))

decompo=str_match(tib_ingredients$ingredients,
                  "(\\w?g|\\w?l|cuillère|CàS|CàC|pot|verre|boîte|pincée|flacon|sachet|feuille|goutte) (de\\s|d\\')(.*)")
tib_ingredients=bind_cols(tib_ingredients,
                          unite=decompo[,2]) %>%
  mutate(ingredients=case_when(!is.na(decompo[,4])~decompo[,4],
                               is.na(decompo[,4])~ingredients))

tib_ingredients %>% 
  select(-url) %>% 
  head()
  
```

## Récupérer la liste des commentaires

On définit maintenant la fonction `recup_commentaires()` qui permet, à partir de l'url d'une recette, de récupérer un tableau qui liste les commentaires (s'il y en a).

```{r recup_commentaires}
recup_commentaires <- function(url){
  url <- str_replace(url,"recettes/recette_","recettes/recette-avis_")
  html <- url %>%
    read_html()
  comments <- html %>% html_nodes(".commentaire")
  if(length(comments)!=0){
      auteur=comments %>% html_nodes("strong") %>% 
        html_text() %>% 
        unlist()
      date <- comments %>% html_nodes(".infoCom") %>%
        html_text() %>% 
        str_extract("\\d{2}/\\d{2}/\\d{2}") %>% 
        unlist()
      note <- comments %>% html_nodes(".bulle") %>%
        html_text()%>%
        str_replace("/5","") %>% 
        unlist()
      texte <- comments %>%
        html_nodes(".txtCommentaire") %>%
        html_text() %>% 
        unlist()
      tib=bind_cols(texte=texte,
                    auteur=auteur,
                    date=date,
                    note=note)
      recette=html %>% 
        html_nodes(".fn") %>% 
        html_text() 
      tib=bind_cols(url=rep(url,nrow(tib)),
                    recette=rep(recette,nrow(tib)),
                    tib)
  }else{tib=NULL}      
  return(tib)
}
```

On applique alors itérativement la fonction `recup_commentaires()` à l'ensemble des recettes listées dans `recettes_sample`.

```{r applique_recup_commentaires}
tib_commentaires <- map(recettes_sample$urls,recup_commentaires) %>% 
    bind_rows()

select(tib_commentaires,-url) %>%
  head(n=15)
```

## Nettoyer la table des commentaires

Je nettoie la table `tib_commentaires`. Je vais notamment m'intéresser à la variable `texte` qui contient un certain nombres d'informations qu'il serait intéressant de distinguer, mais qui ne correspondent pas à différents éléments du html. Je vais donc séparer ces éléments en utilisant des expressions régulières...

Les trois étapes de nettoyage correspondent à  

- remplacement par "_" du pattern "retour-chariot"-"retour à la ligne"-"zero à plusieurs espaces" 
- extraction des caractères (tous caractères sauf "_", zéro à plusieurs fois) suivis par un "\_" et la fin de la chaîne de caractère
- remplacement par un espace des apostrophes ("'" ou "’")

J'écris cette table `tib_commentaires` "clean" dans mon dossier data.

```{r clean_tib_commentaires}
tib_commentaires <- tib_commentaires %>%
  mutate(texte=str_replace_all(texte,"\\r\\n\\s*","_")) %>% 
  mutate(texte=str_extract(texte,"[[^_]]*(?=_$)")) %>% 
  mutate(texte=str_replace_all(texte,"\\'|’"," "))

write_csv(tib_commentaires,"data/tib_commentaires.csv")
```

Je vais aussi juste en écrire une (beaucoup) plus petite pour les exos Datacamp...

```{r}
write_csv(sample_n(tib_commentaires,50),"data/ptib_commentaires.csv")
```




# Traitement du langage naturel (sur les commentaires)


## Tokenization

Je crée la table `tib_mots` à partir de `tib_commentaires`, en tokenisant les commentaires en mots (j'en profite pour lui demander de tout mettre en minuscules avec l'argument `to_lower=TRUE`.

```{r tokenize_instructions}
tib_mots <- unnest_tokens(tib_commentaires,
                          output="word",
                          input="texte",
                          to_lower=TRUE)
write_csv(tib_mots,"data/tib_mots.csv")

tib_mots %>% 
  select(recette,word) %>% 
  head()
```


## Commentaires: mots vides

Je retire les mots vides (en français), et je filtre également les mots pour retirer tous les nombres (pattern `\\d`).

```{r stopwords}
tib_mots_nonvides <- anti_join(tib_mots,
                      proust_stopwords()) %>% 
  filter(!str_detect(word,"\\d+"))

tib_mots_nonvides %>% 
  select(recette,word) %>% 
  head()
```

## Stemming

Je teste la racinisation des mots de `tib_mots`:

```{r stemming}
tib_racines <-  pr_stem_words(tib_mots_nonvides,
                              word)
tib_racines %>% 
  select(recette,word) %>% 
  head()
```

## Lemmatisation

Je peux également tester la lemmatisation. Ici je me suis contentée de lemmatiser en utilisant la base de données Lexique382 (j'ai téléchargé ces données sur [le site de Lexique](http://www.lexique.org/telLexique.php)), puis en réalisant une jointure.

Voici un extrait de la table lexique382 (ici je ne montre que les variables qui m'intéressent ici)

```{r lexique382}
lexique382 <- read.delim("Lexique382/Lexique382.txt", encoding="UTF-8")
lexique382 %>% 
  select(X1_ortho,
         X3_lemme) %>% 
  head(n=20)
```


Un même mot (ou forme) peut correspondre à plusieurs lemmes (par exemple la forme "décevant" peut correspondre à la fois au lemme "décevant" -s'il est employé en tant qu'adjectif- et au lemme "décevoir" -s'il est employé en tant que participe présent-) donc pour ne pas changer la fréquence des mots ici j'ai seulement récupéré un seul des lemmes possibles (le premier apparaissant dans la table).

```{r lexique382_un_lemme_par_forme}
prend_premier=function(x){x[1]}
lexique382 <- lexique382 %>% 
  select(word= X1_ortho,
         lemme= X3_lemme) %>% 
  group_by(word) %>% 
  summarise(lemme=prend_premier(lemme)) %>% 
  distinct()
```

```{r lemmatisation}
tib_mots_nonvides=left_join(tib_mots_nonvides,
                   lexique382,
                   by="word")
write_csv(tib_mots_nonvides,"data/tib_mots_nonvides.csv")
```

## Sentiments

J'associe aux mots une polarité positive ou négative

```{r}
tib_mots_polarite <- tib_mots_nonvides %>%
    left_join(proust_sentiments(), by=c("lemme"="word")) 

tib_mots_polarite  %>%
  group_by(lemme,polarity) %>% 
  summarise(n=n()) %>% 
  arrange(desc(n))
```

Dans le cas des commentaires Marmiton, je trouve que l'association des mots à un "sentiment" ou une "polarité" ne fonctionne pas très bien... Par exemple "gâteau", "recette" et "sucre" sont censés être positifs... autant dire que les commentaires ressortent tous comme étant très positifs...

(Mais je voulais quand-même vous montrer ce qu'il était possible de faire... Pour un vocabulaire un peu moins spécifique et en ayant une problématique un peu mieux définie ça peut peut-être donner des résultats intéressants ;-) )

# Occurrences et visualisation

## Fréquence d'occurrence des mots

Pour passer maintenant aux visualisations les plus "classiques", j'ai besoin de calculer les fréquences d'occurrence des mots, et de trier pour n'en garder qu'une partie... (sinon les graphiques seront clairement surchargés!). 

Je fais cela à la fois sur les mots des ingrédients (= sur les ingrédients => `tib_ingredients`) et sur les mots des commentaires (=> `tib_commentaires`). Heureusement dplyr me permet de ne faire qu'une bouchée de ce calcul.

### Ingredients

```{r freq_ingredients}
freq_ingredients=tib_ingredients %>% 
  group_by(ingredients) %>% 
  summarise(freq=n())  %>% 
  arrange(desc(freq))
head(freq_ingredients)
```

### Commentaires

```{r freq_mots}
freq_mots=tib_mots_nonvides %>% 
  group_by(word) %>% 
  summarise(freq=n()) %>% 
  arrange(desc(freq))
head(freq_mots)
```

```{r}
tib_mots_frequents <- tib_mots_nonvides %>% 
  group_by(word) %>% 
  mutate(freq=n()) %>% 
  filter(freq>100)
write_csv(tib_mots_frequents,"data/tib_mots_frequents.csv")
```



## Nuage de mots

### Ingrédients

La fonction `wordcloud()` attend en arguments d'entrée deux vecteurs: les mots d'une part, et leur fréquence d'autre part.

```{r freq_ingredients_wordcloud}
top_100_ingredients <- top_n(freq_ingredients,100,freq)
wordcloud(words=top_100_ingredients$ingredients,
          freq= top_100_ingredients$freq)
```

### Commentaires

```{r freq_mots_wordcloud}
top_100_mots=top_n(freq_mots,100,freq)
wordcloud(words=top_100_mots$word,
          freq=top_100_mots$freq)
```

## Barplots

Si on veut être en mesure de lire plus directement les fréquences d'occurrence des mots, alors on peut opter pour un graphique plus classique. Par la grâce de ggplot2! hop! un barplot!

### Ingrédients

```{r freq_ingredients_barplot, fig.width=6, fig.height=6}
top_25_ingredients=top_n(freq_ingredients,25, freq)
ggplot(top_25_ingredients, aes(x=forcats::fct_reorder(ingredients,freq), y=freq))+
  geom_bar(stat="identity")+
  coord_flip()+
  xlab("ingrédient")
```

### Commentaires

```{r freq_mots_barplot, fig.width=6, fig.height=10}
top_25_mots=top_n(freq_mots, 25, freq)
ggplot(top_25_mots, aes(x=forcats::fct_reorder(word,freq), y=freq))+
  geom_bar(stat="identity")+
  coord_flip()+
  xlab("mot des commentaires")
```

# Co-occurences

On va maintenant s'intéresser aux co-occurrences de mots. Par exemple, intéressons-nous à la façon dont deux mots tendent à apparaître dans un même commentaire. 

La librairie `widyr` permet d'obtenir les fréquences de co-occurrence et les corrélations deux à deux...

```{r}
tib_mots_filtree=tib_mots_nonvides %>% 
  group_by(lemme) %>%
  mutate(n=n()) %>% 
  filter(n>50) %>% 
  ungroup() 

mots_comptes=tib_mots_filtree %>%
  pairwise_count(lemme,recette,sort=TRUE) 

mots_cors= tib_mots_filtree %>% 
  pairwise_cor(lemme,recette,sort=TRUE)

mots_paires=left_join(mots_comptes,
                      mots_cors,
                      by=c("item1","item2"))
write_csv(mots_paires,"data/mots_paires.csv")
```

## Graphe représentant corrélations entre mots

On filtre les données `tib_mots_paires` pour écarter les paires qui ont soit une faible co-occurrence, soit une faible corrélation. Cela va nous permettre de construire un graphe plus lisible...

En effet, l'ensemble des paires listées dans le tableau `tib_graph` est pris en compte pour construire le layout du graphe... Pour le coup j'ai préféré utiliser les lemmes pour éviter de me retrouver avec des liens entre "bananes" et "banane", "ajouté" et "ajouter", etc. 

```{r, fig.width=8, fig.height=8}
tib_graph=mots_paires %>%
   filter(n>20,
          correlation>0.3,
          item1!="recette",
          item2!="recette") %>%
   select(-n)

graph_from_data_frame(tib_graph) %>%
   ggraph(layout = "fr") +
   geom_edge_link(aes(edge_alpha = correlation),
                  show.legend = FALSE) +
   geom_node_point(color = "lightblue", size = 5) +
   geom_node_text(aes(label = name), repel = TRUE) +
   theme_void()
```


